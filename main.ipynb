{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1 - Aprendizado de Máquina\n",
    "\n",
    "Este arquivo contém o código, bem como as explicações necessárias, para o T1 da disciplina de Aprendizado de Máquina, compreendendo as etapas de download do dataset escolhido, pré-processamento dos dados, treinamento dos modelos de **kNN, Naïve Bayes e Árvores de Decisão** e, por fim, suas avaliações, interpretações e comparações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download do dataset\n",
    "\n",
    "O dataset escolhido para a tarefa de classificação possui dados sobre características físico-químicas da água e classifica as amostras em potável ou não. Nesta seção, fazemos o download do dataset do Kaggle e o convertemos para um objeto `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub as kh\n",
    "import pandas    as pd\n",
    "import os\n",
    "\n",
    "dataset_path = kh.dataset_download(\"uom190346a/water-quality-and-potability\")\n",
    "dataset_file = \"water_potability.csv\"\n",
    "\n",
    "dataset_abs_path = os.path.join(dataset_path, dataset_file)\n",
    "\n",
    "df = pd.read_csv(dataset_abs_path)\n",
    "print(df.head())\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento\n",
    "\n",
    "Para o pré-processamento, serão aplicados:\n",
    "1. Tratamento de valores ausentes (NaNs) preenchendo-os com a mediana da coluna;\n",
    "2. Verificação de outliers e padronização dos dados (para o kNN);\n",
    "3. Separação de atributos (features) e rótulos (labels).\n",
    "\n",
    "Não será realizada a separação em conjuntos de treino e teste pois será utilizada **validação cruzada**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Valores NaN (pré-tratamento):\")\n",
    "print(df.isna().sum())\n",
    "df['ph'] = df['ph'].fillna(df['ph'].median())\n",
    "df['Sulfate'] = df['Sulfate'].fillna(df['Sulfate'].median())\n",
    "df['Trihalomethanes'] = df['Trihalomethanes'].fillna(df['Trihalomethanes'].median())\n",
    "\n",
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    sns.boxplot(data=df, y=col)\n",
    "    plt.title(col)\n",
    "    plt.tight_layout()\n",
    "plt.suptitle(\"Boxplots para detecção de outliers\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "X = df.drop(columns=['Potability'])\n",
    "y = df['Potability']\n",
    "\n",
    "# Padronizacao devido aos outliers para o kNN\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "print(\"Dataset tratado:\")\n",
    "print(X.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento dos modelos (com validação cruzada)\n",
    "\n",
    "Nesta seção é feito o treinamento dos modelos utilizando o método de grid search para encontrar os parâmetros que produzem os melhores valores de acurácia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [2, 5, 10, 15],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'splitter': ['best'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'ccp_alpha': [0.0, 0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3, shuffle=False)\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    param_grid,\n",
    "    cv=kf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "dt_grid.fit(X, y)\n",
    "\n",
    "print(\"Best score (DT):\", dt_grid.best_score_)\n",
    "print(\"Best params (DT):\", dt_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [21, 35, 49,71,83, 105],\n",
    "    'weights': ['distance'],\n",
    "    'algorithm': ['brute'],\n",
    "    'leaf_size': [1,2,3,5,30],\n",
    "    'metric': ['cosine'],\n",
    "    'p': [1, 2], # 1=Manhattan, 2=Euclidean\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3, shuffle=False)\n",
    "knn_grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid,\n",
    "    cv=kf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "knn_grid.fit(X, y)\n",
    "\n",
    "print(\"Best score (kNN):\", knn_grid.best_score_)\n",
    "print(\"Best params (kNN):\", knn_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "param_grid = {\n",
    "    'var_smoothing': np.logspace(-11, -7, 3)\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "nb_grid = GridSearchCV(\n",
    "    estimator=GaussianNB(),\n",
    "    param_grid=param_grid,\n",
    "    cv=kf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "nb_grid.fit(X, y)\n",
    "\n",
    "print(\"Best accuracy (NB):\", nb_grid.best_score_)\n",
    "print(\"Best params (NB):\", nb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretação dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#################################### SHAP #################################### \n",
    "shap.initjs()\n",
    "\n",
    "explainer = shap.TreeExplainer(dt_grid.best_estimator_)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X, feature_names=X.columns)\n",
    "\n",
    "##################################### LIME ####################################\n",
    "X_np = X.values if hasattr(X, \"values\") else X\n",
    "feature_names = X.columns if hasattr(X, \"columns\") else [f\"Feature {i}\" for i in range(X.shape[1])]\n",
    "class_names = np.unique(y).astype(str)\n",
    "\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data=X_np,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    mode=\"classification\"\n",
    ")\n",
    "\n",
    "instance_idx = 0\n",
    "exp = lime_explainer.explain_instance(X_np[instance_idx], dt_grid.predict_proba)\n",
    "\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "plt.figure()\n",
    "exp.as_pyplot_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#################################### SHAP ####################################\n",
    "shap.initjs()\n",
    "\n",
    "X_background = shap.kmeans(X,300)\n",
    "\n",
    "explainer = shap.KernelExplainer(knn_grid.best_estimator_.predict_proba, X_background)\n",
    "shap_values = explainer.shap_values(X.iloc[:50])\n",
    "figure=plt.figure()\n",
    "shap.summary_plot(shap_values, X.iloc[:50])\n",
    "\n",
    "##################################### LIME ####################################\n",
    "X_np = X.values if hasattr(X, \"values\") else X\n",
    "feature_names = X.columns if hasattr(X, \"columns\") else [f\"Feature {i}\" for i in range(X.shape[1])]\n",
    "class_names = np.unique(y).astype(str)\n",
    "\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data=X_np,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    mode=\"classification\"\n",
    ")\n",
    "\n",
    "instance_idx = 0\n",
    "exp = lime_explainer.explain_instance(X_np[instance_idx], knn_grid.predict_proba)\n",
    "\n",
    "exp.show_in_notebook(show_table=True)\n",
    "plt.figure()\n",
    "exp.as_pyplot_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##################################### SHAP ####################################\n",
    "shap.initjs()\n",
    "\n",
    "X_background=shap.sample(X,300)\n",
    "explainer=shap.KernelExplainer(nb_grid.best_estimator_.predict_proba, X_background)\n",
    "shap_values = explainer.shap_values(X.iloc[:50])\n",
    "\n",
    "figure=plt.figure()\n",
    "shap.summary_plot(shap_values, X.iloc[:50])\n",
    "\n",
    "##################################### LIME ####################################\n",
    "X_np = X.values if hasattr(X, \"values\") else X\n",
    "feature_names = X.columns if hasattr(X, \"columns\") else [f\"Feature {i}\" for i in range(X.shape[1])]\n",
    "class_names = np.unique(y).astype(str)\n",
    "\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data=X_np,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    mode=\"classification\"\n",
    ")\n",
    "\n",
    "instance_idx = 0\n",
    "exp = lime_explainer.explain_instance(X_np[instance_idx], nb_grid.predict_proba)\n",
    "\n",
    "exp.show_in_notebook(show_table=True)\n",
    "plt.figure()\n",
    "exp.as_pyplot_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparações de interpretabilidade e limitações dos modelos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
