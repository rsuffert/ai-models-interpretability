{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1 - Aprendizado de Máquina\n",
    "\n",
    "Este arquivo contém o código, bem como as explicações necessárias, para o T1 da disciplina de Aprendizado de Máquina, compreendendo as etapas de download do dataset escolhido, pré-processamento dos dados, treinamento dos modelos de **kNN, Naïve Bayes e Árvores de Decisão** e, por fim, suas avaliações, interpretações e comparações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download do dataset\n",
    "\n",
    "O dataset escolhido para a tarefa de classificação possui dados sobre características físico-químicas da água e classifica as amostras em potável (rótulo 1) ou não-potável (rótulo 0). Nesta seção, fazemos o download do dataset do Kaggle e o convertemos para um objeto `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub as kh\n",
    "import pandas    as pd\n",
    "import os\n",
    "\n",
    "dataset_path = kh.dataset_download(\"uom190346a/water-quality-and-potability\")\n",
    "dataset_file = \"water_potability.csv\"\n",
    "\n",
    "dataset_abs_path = os.path.join(dataset_path, dataset_file)\n",
    "\n",
    "df = pd.read_csv(dataset_abs_path)\n",
    "print(df.head())\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento\n",
    "\n",
    "Para o pré-processamento, serão aplicados:\n",
    "1. Tratamento de valores ausentes (NaNs) preenchendo-os com a mediana da coluna;\n",
    "2. Verificação de outliers e padronização dos dados (para o kNN);\n",
    "3. Separação de atributos (features) e rótulos (labels).\n",
    "\n",
    "Não será realizada a separação em conjuntos de treino e teste pois será utilizada **validação cruzada**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Valores NaN (pré-tratamento):\")\n",
    "print(df.isna().sum())\n",
    "df['ph'] = df['ph'].fillna(df['ph'].median())\n",
    "df['Sulfate'] = df['Sulfate'].fillna(df['Sulfate'].median())\n",
    "df['Trihalomethanes'] = df['Trihalomethanes'].fillna(df['Trihalomethanes'].median())\n",
    "\n",
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    sns.boxplot(data=df, y=col)\n",
    "    plt.title(col)\n",
    "    plt.tight_layout()\n",
    "plt.suptitle(\"Boxplots para detecção de outliers\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "X = df.drop(columns=['Potability'])\n",
    "y = df['Potability']\n",
    "\n",
    "# Padronizacao devido aos outliers para o kNN\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "print(\"Dataset tratado:\")\n",
    "print(X.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento dos modelos (com validação cruzada)\n",
    "\n",
    "Nesta seção é feito o treinamento dos modelos utilizando o método de grid search para encontrar os parâmetros que produzem os melhores valores de acurácia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### TUNING ####################################\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'splitter': ['best'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'ccp_alpha': [0.0, 0.0001, 0.001, 0.005],\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3, shuffle=False)\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    param_grid,\n",
    "    cv=kf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "dt_grid.fit(X, y)\n",
    "\n",
    "print(\"Best grid score (DT):\", dt_grid.best_score_)\n",
    "print(\"Best grid params (DT):\", dt_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### TREINAMENTO (OU CARREGAMENTO) DO MODELO ####################################\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT_MODEL_PATH = os.path.join(\"models\", \"dt-model.joblib\")\n",
    "\n",
    "if not os.path.exists(DT_MODEL_PATH):\n",
    "    dt = DecisionTreeClassifier(\n",
    "        ccp_alpha=0.0001,\n",
    "        class_weight=None,\n",
    "        criterion='gini',\n",
    "        max_depth=5,\n",
    "        max_features='log2',\n",
    "        min_samples_leaf=1,\n",
    "        min_samples_split=10,\n",
    "        splitter='best'\n",
    "    )\n",
    "    dt.fit(X, y)\n",
    "    joblib.dump(dt, DT_MODEL_PATH)\n",
    "\n",
    "dt_model = joblib.load(DT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### TUNING ####################################\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 9, 15, 21, 27],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['minkowski', 'cosine'],\n",
    "    'p': [1, 2], # 1=Manhattan, 2=Euclidean (only used when metric='minkowski')\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3, shuffle=False)\n",
    "knn_grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid,\n",
    "    cv=kf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "knn_grid.fit(X, y)\n",
    "\n",
    "print(\"Best score (kNN):\", knn_grid.best_score_)\n",
    "print(\"Best params (kNN):\", knn_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### TREINAMENTO (OU CARREGAMENTO) DO MODELO ####################################\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN_MODEL_PATH = os.path.join(\"models\", \"knn-model.joblib\")\n",
    "\n",
    "if not os.path.exists(KNN_MODEL_PATH):\n",
    "    knn = KNeighborsClassifier(\n",
    "        metric='minkowski',\n",
    "        n_neighbors=21,\n",
    "        p=2,\n",
    "        weights='distance'\n",
    "    )\n",
    "    knn.fit(X, y)\n",
    "    joblib.dump(knn, KNN_MODEL_PATH)\n",
    "\n",
    "knn_model = joblib.load(KNN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### TUNING ####################################\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "param_grid = {\n",
    "    'var_smoothing': np.logspace(-11, -7, 3)\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "nb_grid = GridSearchCV(\n",
    "    estimator=GaussianNB(),\n",
    "    param_grid=param_grid,\n",
    "    cv=kf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "nb_grid.fit(X, y)\n",
    "\n",
    "print(\"Best accuracy (NB):\", nb_grid.best_score_)\n",
    "print(\"Best params (NB):\", nb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### TREINAMENTO (OU CARREGAMENTO) DO MODELO ####################################\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "NB_MODEL_PATH = os.path.join(\"models\", \"nb-model.joblib\")\n",
    "\n",
    "if not os.path.exists(NB_MODEL_PATH):\n",
    "    nb = GaussianNB(var_smoothing=1e-11)\n",
    "    nb.fit(X, y)\n",
    "    joblib.dump(nb, NB_MODEL_PATH)\n",
    "\n",
    "nb_model = joblib.load(NB_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretação dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árvore de Decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a interpretação da Árvore de Decisão gerada, será analizado o **SHAP summary** das features e, então, será verificado como as observações feitas nesse gráfico se refletem na representação visual da árvore. É importante ressaltar que o SHAP já ordena as features em ordem decrescente, de cima para baixo, baseado na importância que aquela feature tem para as decisões feitas pelo modelo.\n",
    "\n",
    "Algumas observações pertinentes com relação à forma como o modelo interpreta as features do dataset:\n",
    "\n",
    "Com relação ao **SHAP summary**:\n",
    "- **Dureza e Sulfatos**: São as features mais importantes. Valores baixos tendem a puxar as predições para a classe positiva (1), enquanto valores altos por si só tendem a não ser tão conclusivos para o modelo. A dureza possui alguns outliers com alto valor e que tiveram forte influência para predizer a classe positiva.\n",
    "- **pH**: De um modo geral, valores baixos (ácidos) tendem a puxar as predições do modelo para a classe negativa (0), mas também podem influenciar até um certo limite na predição para a classe positiva. O mesmo vale para valores altos de pH, mas estes tendem a ter um impacto menos significativo nas predições.\n",
    "- **Sólidos**: Tendem a contribuir para as predições, mas com impactos mais variados com relação aos seus valores absolutos. Há uma sobreposição mais significativa entre os diferentes valores assumidos pela feature e os SHAP values considerados, indicando que essa feature pode ter uma maior dependência com os valores de outras para definir o resultado da classificação.\n",
    "- **Cloraminas**: É visível que, quando esses valores contribuem para uma predição a favor da classe positiva (1), são baixos. Há uma leve tendência, também, a classificar valores altos como favoráveis à classe negativa (0).\n",
    "- **Carbonos Orgânicos, Trialometanos, Turbidez e Condutividade**: De um modo geral, apresentam um padrão semelhante no SHAP summary, mas não são considerados muito relevantes pelo modelo. Nos poucos casos em que apresentam alguma contribuição para a predição, seus altos valores tendem a influenciar a classificação para a classe negativa (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#################################### SHAP ####################################\n",
    "import shap\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "explainer = shap.TreeExplainer(dt_model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "#for class_idx in range(shap_values.shape[2]):\n",
    "#    print(f\"SHAP summary plot for class {class_idx}:\")\n",
    "#    plt.figure()\n",
    "#    shap.summary_plot(shap_values[:, :, class_idx], X, feature_names=X.columns)\n",
    "\n",
    "# Plotamos apenas a classe 0, ja que eh um problema de classificacao binaria\n",
    "print(f\"SHAP summary plot for class 1 (POTABLE):\")\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values[:, :, 1], X, feature_names=X.columns)\n",
    "\n",
    "#################################### DT ####################################\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "print(\"Generated decision tree:\")\n",
    "plt.figure(figsize=(30, 15))\n",
    "plot_tree(\n",
    "    dt_model,\n",
    "    feature_names=X.columns,\n",
    "    class_names=np.unique(y).astype(str),\n",
    "    filled=True,\n",
    "    proportion=True,\n",
    "    precision=2\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Usar o modelo final knn_model em vez de knn_grid\n",
    "\n",
    "#################################### SHAP ####################################\n",
    "shap.initjs()\n",
    "\n",
    "X_background = shap.kmeans(X,300)\n",
    "\n",
    "explainer = shap.KernelExplainer(knn_grid.best_estimator_.predict_proba, X_background)\n",
    "shap_values = explainer.shap_values(X.iloc[:50])\n",
    "figure=plt.figure()\n",
    "shap.summary_plot(shap_values, X.iloc[:50])\n",
    "\n",
    "##################################### LIME ####################################\n",
    "X_np = X.values if hasattr(X, \"values\") else X\n",
    "feature_names = X.columns if hasattr(X, \"columns\") else [f\"Feature {i}\" for i in range(X.shape[1])]\n",
    "class_names = np.unique(y).astype(str)\n",
    "\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data=X_np,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    mode=\"classification\"\n",
    ")\n",
    "\n",
    "instance_idx = 0\n",
    "exp = lime_explainer.explain_instance(X_np[instance_idx], knn_grid.predict_proba)\n",
    "\n",
    "exp.show_in_notebook(show_table=True)\n",
    "plt.figure()\n",
    "exp.as_pyplot_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Usar o modelo final nb_model em vez de nb_grid\n",
    "\n",
    "##################################### SHAP ####################################\n",
    "shap.initjs()\n",
    "\n",
    "X_background=shap.sample(X,300)\n",
    "explainer=shap.KernelExplainer(nb_grid.best_estimator_.predict_proba, X_background)\n",
    "shap_values = explainer.shap_values(X.iloc[:50])\n",
    "\n",
    "figure=plt.figure()\n",
    "shap.summary_plot(shap_values, X.iloc[:50])\n",
    "\n",
    "##################################### LIME ####################################\n",
    "X_np = X.values if hasattr(X, \"values\") else X\n",
    "feature_names = X.columns if hasattr(X, \"columns\") else [f\"Feature {i}\" for i in range(X.shape[1])]\n",
    "class_names = np.unique(y).astype(str)\n",
    "\n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data=X_np,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    mode=\"classification\"\n",
    ")\n",
    "\n",
    "instance_idx = 0\n",
    "exp = lime_explainer.explain_instance(X_np[instance_idx], nb_grid.predict_proba)\n",
    "\n",
    "exp.show_in_notebook(show_table=True)\n",
    "plt.figure()\n",
    "exp.as_pyplot_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparações de interpretabilidade e limitações dos modelos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
